{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green>ISAT 449: Emerging Topics in Applied Data Science</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>Mini-Project: How to Make a Speech Emotion Recognizer Using Python And Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>**Building a Speech Emotion Recognition system that detects emotion from a human speech tone using the Scikit-Learn library, Python, and Librosa**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Speech Emotion Recognition?**\n",
    "\n",
    "Speech Emotion Recognition (SER) is the act of attempting to recognize human emotion and affective states from speech. This is capitalizing on the fact that voice often reflects underlying emotion through tone and pitch. This is also the phenomenon that animals like dogs and horses employ to be able to understand human emotion.\n",
    "\n",
    "SER is tough because emotions are subjective and annotating audio is challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is librosa?**\n",
    "\n",
    "Librosa is a python library for analyzing audio and music. It has a flatter package layout, standardizes interfaces and names, backwards compatibility, modular functions, and readable code. Further, in this Python mini-project, we demonstrate how to install it (and a few other packages) with pip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is JupyterLab?**\n",
    "\n",
    "As you have seen, I use JupyterLab, which is an open-source, web-based UI for Project Jupyter and it has all basic functionalities of the Jupyter Notebook, like notebooks, terminals, text editors, file browsers, rich outputs, and more. However, if also provided support for third party extensions. It comes bundled with the Anaconda Data Science Framework if you want to try it out, BUT you can just keep using your Jupyter Notebook and all will be fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speech Emotion Recognition - Objective**\n",
    "\n",
    "To build a model to recognize emotion from speech using the librosa and sklearn libraries and the RAVDESS dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speech Emotion Recognition - About the Python Mini Project**\n",
    "\n",
    "In this Python mini project, we will uset the libraries librosa, soundfile, and sklearn (among others) to build a model using an MLPClassifier. This will be able to recognize emotion from sound files. We will load the data, extract features from it, then split the dataset into training and testing sets. Then, we'll initialize an MLPClassifier and train the model. Finally, we'll calculate the accuracy of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Dataset**\n",
    "\n",
    "For this Python mini project, we'll use the RAVDESS dataset; this is the Ryerson Audio-Visual Dataset of Emotional Speech and Song dataset, and is free to download. This dataset has 7356 files rated by 247 individuals 10 times on emotional validity, intensity, and genuineness. The entire dataset is 24.8GB from 24 actors, but we've lowered the sample rate on all the files, and you can download it from Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**File Summary**\n",
    "\n",
    "In total, the RAVDESS collection includes 7356 files (2880 + 2024 + 1440 + 1012 files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**File naming convention**\n",
    "\n",
    "Each of the 7356 RAVDESS files has unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filename Identifiers**\n",
    "\n",
    " - Modalitiy (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    " - Vocal channel (01 = speech, 02 = song).\n",
    " - Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    " - Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    " - Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    " - Repetition (01 = 1st repitition, 02 = 2nd repitition).\n",
    " - Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    " \n",
    "_Filename example: 02-01-06-01-02-01-12.mp4_\n",
    "\n",
    " - Video-only (02)\n",
    " - Speech (01)\n",
    " - Fearful (06)\n",
    " - Normal intensity (01)\n",
    " - Statement \"dogs\" (02)\n",
    " - 1st Repetition (01)\n",
    " - 12th Actor (12)\n",
    " - Female, as the actor ID number is even.\n",
    " \n",
    "You can find more information on the file structure and filenames from Zenodo: Filename References\n",
    "(https://zenodo.org/record/1188976#.X3KzGGhKhPa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prerequisites**\n",
    "\n",
    "You'll need to install the following libraries with pip:\n",
    "\n",
    " - **pip install** _librosa soundfile numpy sklearn pyaudio_\n",
    " \n",
    "If you run into issues installing librosa with **pip**, you can try it with **conda**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole pipeline is as follows (same as any machine learning pipeline):\n",
    " - Preparing the Dataset: Here, we download and convert the dataset to be suited for extraction.\n",
    " - Loading the Dataset: This process is about loading the dataset in Python which involves extracting audio features, such as obtaining different features (power, pitch, and vocal tract configuration from the speech signal). We will use _librosa_ library to do that.\n",
    " - Training the Model: After we prepare and load the dataset, we simply train it on a suited sklearn model.\n",
    " - Testing the Model: Measuring how good our model is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NOTE on the dataset and file structure**\n",
    "\n",
    "<font color=red>**BEFORE you begin coding, download the dataset from canvas and _extract it to your project folder_. I have modified the folder name in the zipped file so that it is easier to parse (see the file structure I used below).**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's import the dependencies\n",
    "\n",
    " 1. import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile # to read audio file\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa # to extract speech features\n",
    "import glob\n",
    "import os\n",
    "import pickle # to save model after training\n",
    "\n",
    "from sklearn.model_selection import train_test_split # for splitting training and testing\n",
    "from sklearn.neural_network import MLPClassifier # multi-layer perceptron model\n",
    "from sklearn.metrics import accuracy_score # to measure how good we are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. Define a function extract_feature to extract the **mfcc**, **chroma**, and **mel** features from a sound file. This function takes 4 parameters - the file name and three Boolean parameters for the three features:\n",
    "    1. **mfcc**: Mel Frequency Cepstral Coeffient, represents the short-term power spectrum of a sound\n",
    "    2. **chroma**: Pertains to the 12 different pitch classes\n",
    "    3. **mel**: Mel Spectrogram Frequency\n",
    "    \n",
    "Open the sound file with soundfile.SoundFile using with-as so it's automatically closed once we're done. Read from it and call it X. Also, get the sample rate. If chroma is True, get the Short-Time Fourier Transform of X.\n",
    "\n",
    "Let result be an empty numpy array. Now, for each feature of the three, if it exists, make a call to the corresponding function from librosa.feature (e.g., librosa.feature.mfcc for mfcc), and get the mean value. Call the function hstack() from numpy with result and the feature value, and store this in result. hstack() stacks arrays in sequence horizontally (in a columnar fashion). Then, return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (mfcc, chroma, mel) from a sound file\n",
    "def extract_features(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft = np.abs(librosa.stft(X))\n",
    "        result = np.array([])\n",
    "        if mfcc:\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result = np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "            result = np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T, axis=0)\n",
    "            result = np.hstack((result, mel))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 3. Now, let's define a dictionary to hold numbers and the emotions available in the RAVDESS dataset, and a list to hold those we want - calm, happy, fearful, disgust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions in the RAVDESS dataset\n",
    "emotions={\n",
    "    '01':'neutral',\n",
    "    '02':'calm',\n",
    "    '03':'happy',\n",
    "    '04':'sad',\n",
    "    '05':'angry',\n",
    "    '06':'fearful',\n",
    "    '07':'disgust',\n",
    "    '08':'surprised'\n",
    "}\n",
    "\n",
    "# Emotions to observe\n",
    "observed_emotions = ['calm', 'happy', 'fearful', 'disgust']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4. Now, let's load the data with a function load_data() - this takes in the relative size of the test set as parameter. x and y are empty lists; We'll use the glob() function from the glob module to get all the pathnames for the sound files in our dataset. The pattern we use for this is: **ravdess-data\\Actor_1.wav**. So, for each such path, get the basename of the file, the emotion by splitting the name around '-' and extracting the third value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and extract features for each sound file\n",
    "def load_data(test_size=0.2):\n",
    "    x, y = [], []\n",
    "    for file in glob.glob(\"ravdess-data\\\\Actor_*\\\\*.wav\"):\n",
    "        file_name = os.path.basename(file)\n",
    "        emotion = emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        feature = extract_features(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Using our emotions dictionary, this number is turned into an emotion, and our function checks whether this emotion is in our list of observed_emotions; if not, it continues to the next file. It makes a call to extract_feature and stores what is returned in 'feature'. Then, it appends the feature to x and the emotion to y. So, the list x holds the features and y holds the emotions.**_\n",
    "\n",
    "We call the function *train_test_split* with these, the *test_size*, and a *random_state* value, and return that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 5. Time to split the dataset into training and testing sets! Let's keep the test st 25% of everything and use the load_data function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "x_train,x_test,y_train,y_test=load_data(test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 6. Observe the shape of the training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 192)\n"
     ]
    }
   ],
   "source": [
    "# Get the shape of the training and testing datasets\n",
    "print((x_train.shape[0], x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 7. And get the number of features extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted: 180\n"
     ]
    }
   ],
   "source": [
    "# Get the number of features extracted\n",
    "print(f'Features extracted: {x_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
